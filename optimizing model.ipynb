{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23c251e-ca93-4ca3-9a65-5f5c01dfbe13",
   "metadata": {},
   "source": [
    "## Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f077b32-5d36-44b0-9041-407e996283a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping optimum as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping optimum-intel as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"llama-index\" \"faiss-cpu\" \"pymupdf\" \"llama-index-readers-file\" \"llama-index-vector-stores-faiss\" \"llama-index-llms-langchain\" \"llama-index-llms-openvino\" \"llama-index-embeddings-openvino\" \"llama-index-postprocessor-openvino-rerank\" \"transformers>=4.40\" \\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "\"datasets\"\\\n",
    "\"accelerate\"\\\n",
    "\"gradio\" \\\n",
    "\"langchain\"\n",
    "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5301736c-ec15-4597-8120-ed809d5902c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.51-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.1.26-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-vector-stores-faiss\n",
      "  Downloading llama_index_vector_stores_faiss-0.1.2-py3-none-any.whl.metadata (660 bytes)\n",
      "Collecting llama-index-llms-langchain\n",
      "  Downloading llama_index_llms_langchain-0.1.4-py3-none-any.whl.metadata (751 bytes)\n",
      "Collecting llama-index-llms-openvino\n",
      "  Downloading llama_index_llms_openvino-0.1.1-py3-none-any.whl.metadata (751 bytes)\n",
      "Collecting llama-index-embeddings-openvino\n",
      "  Downloading llama_index_embeddings_openvino-0.1.7-py3-none-any.whl.metadata (790 bytes)\n",
      "Collecting llama-index-postprocessor-openvino-rerank\n",
      "  Downloading llama_index_postprocessor_openvino_rerank-0.1.6-py3-none-any.whl.metadata (792 bytes)\n",
      "Requirement already satisfied: transformers>=4.40 in /home/saicharan/.local/lib/python3.10/site-packages (4.41.2)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core==0.10.51 (from llama-index)\n",
      "  Downloading llama_index_core-0.10.51-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.2.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.1.24-py3-none-any.whl.metadata (610 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/saicharan/.local/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.51->llama-index) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (1.2.14)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.51->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (0.27.0)\n",
      "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core==0.10.51->llama-index)\n",
      "  Downloading llama_cloud-0.0.6-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (1.26.3)\n",
      "Collecting openai>=1.1.0 (from llama-index-core==0.10.51->llama-index)\n",
      "  Downloading openai-1.35.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pandas in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/lib/python3/dist-packages (from llama-index-core==0.10.51->llama-index) (9.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (8.4.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-core==0.10.51->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/lib/python3/dist-packages (from llama-index-core==0.10.51->llama-index) (1.13.3)\n",
      "Requirement already satisfied: packaging in /home/saicharan/.local/lib/python3.10/site-packages (from faiss-cpu) (23.2)\n",
      "Collecting PyMuPDFb==1.24.6 (from pymupdf)\n",
      "  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-readers-file) (4.2.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain<0.2.0,>=0.1.3 (from llama-index-llms-langchain)\n",
      "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting llama-index-llms-anyscale<0.2.0,>=0.1.1 (from llama-index-llms-langchain)\n",
      "  Downloading llama_index_llms_anyscale-0.1.4-py3-none-any.whl.metadata (647 bytes)\n",
      "Collecting llama-index-llms-huggingface<0.3.0,>=0.2.4 (from llama-index-llms-openvino)\n",
      "  Downloading llama_index_llms_huggingface-0.2.4-py3-none-any.whl.metadata (841 bytes)\n",
      "Requirement already satisfied: optimum>=1.18.0 in /home/saicharan/.local/lib/python3.10/site-packages (from optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.20.0)\n",
      "Collecting llama-index-embeddings-huggingface<0.3.0,>=0.2.2 (from llama-index-embeddings-openvino)\n",
      "  Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl.metadata (769 bytes)\n",
      "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-postprocessor-openvino-rerank) (0.23.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers>=4.40) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/saicharan/.local/lib/python3.10/site-packages (from transformers>=4.40) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/saicharan/.local/lib/python3.10/site-packages (from transformers>=4.40) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/saicharan/.local/lib/python3.10/site-packages (from transformers>=4.40) (0.4.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/saicharan/.local/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/saicharan/.local/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (4.0.3)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain)\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/saicharan/.local/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.1.82)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/saicharan/.local/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.6.4)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-embeddings-huggingface<0.3.0,>=0.2.2->llama-index-embeddings-openvino) (3.0.1)\n",
      "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino)\n",
      "  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /home/saicharan/.local/lib/python3.10/site-packages (from llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (2.2.1)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
      "  Downloading llama_parse-0.4.5-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: coloredlogs in /home/saicharan/.local/lib/python3.10/site-packages (from optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/saicharan/.local/lib/python3.10/site-packages (from optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.12)\n",
      "Requirement already satisfied: datasets in /home/saicharan/.local/lib/python3.10/site-packages (from optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (2.18.0)\n",
      "Requirement already satisfied: optimum-intel>=1.16.0 in /home/saicharan/.local/lib/python3.10/site-packages (from optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.19.0.dev0+60532db)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/saicharan/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/saicharan/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/saicharan/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/saicharan/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.51->llama-index) (2024.6.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/saicharan/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/saicharan/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/saicharan/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/saicharan/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/saicharan/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.51->llama-index) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/saicharan/.local/lib/python3.10/site-packages (from dataclasses-json->llama-index-core==0.10.51->llama-index) (3.21.3)\n",
      "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface<0.3.0,>=0.2.2->llama-index-embeddings-openvino)\n",
      "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/saicharan/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/saicharan/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.10.5)\n",
      "Requirement already satisfied: anyio in /home/saicharan/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.51->llama-index) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/saicharan/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.51->llama-index) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/saicharan/.local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.51->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/saicharan/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.51->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in /home/saicharan/.local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.51->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/saicharan/.local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.51->llama-index) (1.3.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.51->llama-index) (1.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/saicharan/.local/lib/python3.10/site-packages (from optimum-intel>=1.16.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from optimum-intel>=1.16.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (59.6.0)\n",
      "Requirement already satisfied: scipy in /home/saicharan/.local/lib/python3.10/site-packages (from optimum-intel>=1.16.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.12.0)\n",
      "Requirement already satisfied: onnx in /home/saicharan/.local/lib/python3.10/site-packages (from optimum-intel>=1.16.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.16.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/saicharan/.local/lib/python3.10/site-packages (from datasets->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/saicharan/.local/lib/python3.10/site-packages (from datasets->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/saicharan/.local/lib/python3.10/site-packages (from datasets->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/saicharan/.local/lib/python3.10/site-packages (from datasets->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/saicharan/.local/lib/python3.10/site-packages (from datasets->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.70.16)\n",
      "Collecting nncf>=2.11.0 (from optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino)\n",
      "  Downloading nncf-2.11.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: openvino>=2023.3 in /home/saicharan/.local/lib/python3.10/site-packages (from optimum-intel[openvino]>=1.16.0; extra == \"openvino\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (2024.3.0.dev20240617)\n",
      "Requirement already satisfied: openvino-tokenizers[transformers] in /home/saicharan/.local/lib/python3.10/site-packages (from optimum-intel[openvino]>=1.16.0; extra == \"openvino\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (2024.3.0.0.dev20240617)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/saicharan/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/saicharan/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.16.3)\n",
      "Requirement already satisfied: scikit-learn in /home/saicharan/.local/lib/python3.10/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface<0.3.0,>=0.2.2->llama-index-embeddings-openvino) (1.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/saicharan/.local/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.51->llama-index) (3.0.3)\n",
      "Requirement already satisfied: jinja2 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/saicharan/.local/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/saicharan/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (12.4.99)\n",
      "Requirement already satisfied: protobuf in /home/saicharan/.local/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (4.25.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/saicharan/.local/lib/python3.10/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (0.31.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/saicharan/.local/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.51->llama-index) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/saicharan/.local/lib/python3.10/site-packages (from coloredlogs->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/saicharan/.local/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.51->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->llama-index-core==0.10.51->llama-index) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/saicharan/.local/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.51->llama-index) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/saicharan/.local/lib/python3.10/site-packages (from sympy->optimum>=1.18.0->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/saicharan/.local/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (5.9.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/saicharan/.local/lib/python3.10/site-packages (from anyio->httpx->llama-index-core==0.10.51->llama-index) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/saicharan/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.4)\n",
      "Requirement already satisfied: jsonschema>=3.2.0 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (4.20.0)\n",
      "Requirement already satisfied: jstyleson>=0.0.2 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.0.2)\n",
      "Requirement already satisfied: natsort>=7.1.0 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (8.4.0)\n",
      "Requirement already satisfied: ninja<1.12,>=1.10.0.post2 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.11.1.1)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.0 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (2024.1.0)\n",
      "Requirement already satisfied: pydot>=1.4.1 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (2.0.0)\n",
      "Requirement already satisfied: pymoo>=0.6.0.1 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.6.1.1)\n",
      "Requirement already satisfied: rich>=13.5.2 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (13.7.1)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /home/saicharan/.local/lib/python3.10/site-packages (from nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.51->llama-index) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/saicharan/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface<0.3.0,>=0.2.2->llama-index-embeddings-openvino) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/saicharan/.local/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface<0.3.0,>=0.2.4->llama-index-llms-openvino) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/saicharan/.local/lib/python3.10/site-packages (from jsonschema>=3.2.0->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/saicharan/.local/lib/python3.10/site-packages (from jsonschema>=3.2.0->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/saicharan/.local/lib/python3.10/site-packages (from jsonschema>=3.2.0->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.16.2)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/saicharan/.local/lib/python3.10/site-packages (from pydot>=1.4.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (3.1.2)\n",
      "Requirement already satisfied: matplotlib>=3 in /home/saicharan/.local/lib/python3.10/site-packages (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (3.8.2)\n",
      "Requirement already satisfied: autograd>=1.4 in /home/saicharan/.local/lib/python3.10/site-packages (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.6.2)\n",
      "Requirement already satisfied: cma==3.2.2 in /home/saicharan/.local/lib/python3.10/site-packages (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (3.2.2)\n",
      "Requirement already satisfied: alive-progress in /home/saicharan/.local/lib/python3.10/site-packages (from pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (3.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/saicharan/.local/lib/python3.10/site-packages (from rich>=13.5.2->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/saicharan/.local/lib/python3.10/site-packages (from rich>=13.5.2->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (2.17.2)\n",
      "Requirement already satisfied: future>=0.15.2 in /home/saicharan/.local/lib/python3.10/site-packages (from autograd>=1.4->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/saicharan/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/saicharan/.local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/saicharan/.local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/saicharan/.local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/saicharan/.local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (1.4.5)\n",
      "Requirement already satisfied: about-time==4.2.1 in /home/saicharan/.local/lib/python3.10/site-packages (from alive-progress->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (4.2.1)\n",
      "Requirement already satisfied: grapheme==0.6.0 in /home/saicharan/.local/lib/python3.10/site-packages (from alive-progress->pymoo>=0.6.0.1->nncf>=2.11.0->optimum-intel[nncf]>=1.16.0; extra == \"nncf\"->optimum[nncf,openvino]>=1.18.0->llama-index-llms-openvino) (0.6.0)\n",
      "Downloading llama_index-0.10.51-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_core-0.10.51-py3-none-any.whl (15.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_readers_file-0.1.26-py3-none-any.whl (37 kB)\n",
      "Downloading llama_index_vector_stores_faiss-0.1.2-py3-none-any.whl (3.6 kB)\n",
      "Downloading llama_index_llms_langchain-0.1.4-py3-none-any.whl (4.8 kB)\n",
      "Downloading llama_index_llms_openvino-0.1.1-py3-none-any.whl (3.8 kB)\n",
      "Downloading llama_index_embeddings_openvino-0.1.7-py3-none-any.whl (3.7 kB)\n",
      "Downloading llama_index_postprocessor_openvino_rerank-0.1.6-py3-none-any.whl (3.9 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.2.3-py3-none-any.whl (9.2 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_anyscale-0.1.4-py3-none-any.whl (4.2 kB)\n",
      "Downloading llama_index_llms_huggingface-0.2.4-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_llms_openai-0.1.24-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.4.5-py3-none-any.whl (9.1 kB)\n",
      "Downloading openai-1.35.9-py3-none-any.whl (328 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nncf-2.11.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: libtorrent 2.0.5-build-libtorrent-rasterbar-qrM5vM-libtorrent-rasterbar-2.0.5-bindings-python has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of libtorrent or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: striprtf, dirtyjson, PyMuPDFb, minijinja, faiss-cpu, beautifulsoup4, pymupdf, text-generation, openai, llama-cloud, nncf, llama-index-legacy, llama-index-core, langchain-core, llama-parse, llama-index-vector-stores-faiss, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain-text-splitters, langchain-community, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-llms-huggingface, llama-index-llms-anyscale, llama-index-embeddings-huggingface, llama-index-cli, llama-index-agent-openai, langchain, llama-index-program-openai, llama-index-llms-langchain, llama-index-question-gen-openai, llama-index, llama-index-postprocessor-openvino-rerank, llama-index-llms-openvino, llama-index-embeddings-openvino\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.2\n",
      "    Uninstalling beautifulsoup4-4.12.2:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.2\n",
      "  Attempting uninstall: nncf\n",
      "    Found existing installation: nncf 2.11.0.dev0+0c22c2c1\n",
      "    Uninstalling nncf-2.11.0.dev0+0c22c2c1:\n",
      "      Successfully uninstalled nncf-2.11.0.dev0+0c22c2c1\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.10\n",
      "    Uninstalling langchain-core-0.2.10:\n",
      "      Successfully uninstalled langchain-core-0.2.10\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.2\n",
      "    Uninstalling langchain-text-splitters-0.2.2:\n",
      "      Successfully uninstalled langchain-text-splitters-0.2.2\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.6\n",
      "    Uninstalling langchain-0.2.6:\n",
      "      Successfully uninstalled langchain-0.2.6\n",
      "Successfully installed PyMuPDFb-1.24.6 beautifulsoup4-4.12.3 dirtyjson-1.0.8 faiss-cpu-1.8.0.post1 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 llama-cloud-0.0.6 llama-index-0.10.51 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.51 llama-index-embeddings-huggingface-0.2.2 llama-index-embeddings-openai-0.1.10 llama-index-embeddings-openvino-0.1.7 llama-index-indices-managed-llama-cloud-0.2.3 llama-index-legacy-0.9.48 llama-index-llms-anyscale-0.1.4 llama-index-llms-huggingface-0.2.4 llama-index-llms-langchain-0.1.4 llama-index-llms-openai-0.1.24 llama-index-llms-openvino-0.1.1 llama-index-multi-modal-llms-openai-0.1.6 llama-index-postprocessor-openvino-rerank-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.26 llama-index-readers-llama-parse-0.1.4 llama-index-vector-stores-faiss-0.1.2 llama-parse-0.4.5 minijinja-2.0.1 nncf-2.11.0 openai-1.35.9 pymupdf-1.24.7 striprtf-0.0.26 text-generation-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"llama-index\" \"faiss-cpu\" \"pymupdf\" \"llama-index-readers-file\" \"llama-index-vector-stores-faiss\" \"llama-index-llms-langchain\" \"llama-index-llms-openvino\" \"llama-index-embeddings-openvino\" \"llama-index-postprocessor-openvino-rerank\" \"transformers>=4.40\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c73de-c856-417c-960d-86f71cf42e82",
   "metadata": {},
   "source": [
    "## Import required libraries and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2c3f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM config will be updated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# fetch model configuration\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b57cfb-e727-43a5-b2c9-8f1b1ba72061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bf49d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d206f1cc8e9e4b54aec940c865acbd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_config import (\n",
    "    SUPPORTED_EMBEDDING_MODELS,\n",
    "    SUPPORTED_RERANK_MODELS,\n",
    "    SUPPORTED_LLM_MODELS,\n",
    ")\n",
    "\n",
    "model_language = \"English\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184d1678-0e73-4f35-8af5-1a7d291c2e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794acd14e346417e81b44f9c7c442f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=12, options=('tiny-llama-1b-chat', 'gemma-2b-it', 'red-pajama-3b-chat', '…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_id = \"gemma-2b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820861a-211d-49b3-8e72-0694dfea8a8f",
   "metadata": {},
   "source": [
    "## using openvino convert to openvino format and quantizing model to int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a38153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d484224eaab4a5a97b2ef61bf779b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a59f9c7c91343a48754f4b850a8e5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1654076e952d40b985dc9253a84cc6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "convert_to_int8()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1d1d1a2",
   "metadata": {},
   "source": [
    "Let's compare model size for different compression types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e127215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model with INT8 compressed weights is 2393.05 MB\n"
     ]
    }
   ],
   "source": [
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "\n",
    "for precision, compressed_weights in zip([8], [int8_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c6a05-e9b1-4117-88cd-d3c95bfc6e96",
   "metadata": {},
   "source": [
    "## embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c28d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3d6f20eaf8481e8c20515ee27520f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Embedding Model:', options=('bge-small-en-v1.5', 'bge-large-en-v1.5', 'bge-m3'), value='…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_id = \"bge-small-en-v1.5 model\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2fbc403-1cac-4864-8965-ad2fea0fd1ca",
   "metadata": {},
   "source": [
    "OpenVINO embedding model and tokenizer can be exported by `feature-extraction` task with `optimum-cli`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff80e6eb-7923-40ef-93d8-5e6c56e50667",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_command_base = \"optimum-cli export openvino --model {} --task feature-extraction\".format(embedding_model_configuration[\"model_id\"])\n",
    "export_command = export_command_base + \" \" + str(embedding_model_id.value)\n",
    "\n",
    "if not Path(embedding_model_id.value).exists():\n",
    "    ! $export_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40050dc6-e202-4d2a-ab7c-a30da7dfd7d6",
   "metadata": {},
   "source": [
    "## Rerank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5b8840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d90768797b64999b38a8905491b3501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Rerank Model:', options=('bge-reranker-v2-m3', 'bge-reranker-large', 'bge-reranker-base'…"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_model_id = \"bge-reranker-large model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bab20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_command_base = \"optimum-cli export openvino --model {} --task text-classification\".format(rerank_model_configuration[\"model_id\"])\n",
    "export_command = export_command_base + \" \" + str(rerank_model_id.value)\n",
    "\n",
    "if not Path(rerank_model_id.value).exists():\n",
    "    ! $export_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11e73cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d7f0d02d704448bdae3f339acab612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "support_devices = core.available_devices\n",
    "\n",
    "embedding_device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "embedding_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ab29b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model will be loaded to CPU device for text embedding\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding model will be loaded to {embedding_device.value} device for text embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2644c",
   "metadata": {},
   "source": [
    "### Select device for rerank model inference\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0a2586b-5811-420f-834a-2b68de207df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162d609a385b4618b3990358f87cdb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "rerank_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b7a76b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerenk model will be loaded to CPU device for text reranking\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rerenk model will be loaded to {rerank_device.value} device for text reranking\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef31656a",
   "metadata": {},
   "source": [
    "### Select device for LLM model inference\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d044d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d383b5378061434fa7a5b9621cb7bfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "348b90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model will be loaded to CPU device for response generation\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLM model will be loaded to {llm_device.value} device for response generation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc225391",
   "metadata": {},
   "source": [
    "## Load models\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Load embedding model\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now a Hugging Face embedding model can be supported by OpenVINO through [`OpenVINOEmbeddings`](https://docs.llamaindex.ai/en/stable/examples/embeddings/openvino/) class of LlamaIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df3e8fd1-d4c1-4e33-b46e-7840e392f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 10:46:20.806937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 10:46:21.548673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Compiling the model to CPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.0032756736036390066, -0.011690771207213402, 0.04155924171209335, -0.038148146122694016, 0.024183060973882675]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "\n",
    "embedding = OpenVINOEmbedding(folder_name=embedding_model_id.value, device=embedding_device.value)\n",
    "\n",
    "embeddings = embedding.get_text_embedding(\"Hello World!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1a1fd58",
   "metadata": {},
   "source": [
    "### Load rerank model\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now a Hugging Face embedding model can be supported by OpenVINO through [`OpenVINORerank`](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/openvino_rerank/) class of LlamaIndex.\n",
    "\n",
    "> **Note**: Rerank can be skipped in RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b67b39f2-8394-45fb-9b2b-ea63e267a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.postprocessor.openvino_rerank import OpenVINORerank\n",
    "reranker = OpenVINORerank(model=rerank_model_id.value, device=rerank_device.value, top_n=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79fe990a",
   "metadata": {},
   "source": [
    "### Load LLM model\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "OpenVINO models can be run locally through the `HuggingFacePipeline` class. To deploy a model with OpenVINO, you can specify the `backend=\"openvino\"` parameter to trigger OpenVINO as backend inference framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90b968f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c0f583dc774f5b9fb2358fc343fd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT8',), value='INT8')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = []\n",
    "available_models.append(\"INT8\")\n",
    "\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20fcc33c",
   "metadata": {},
   "source": [
    "OpenVINO models can be run locally through the `OpenVINOLLM` class in [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/llm/openvino/). If you have an Intel GPU, you can specify `device_map=\"gpu\"` to run inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f708db-8de1-4efd-94b2-fcabc48d52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "\n",
    "model_dir = int8_model_dir\n",
    "\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "if \"GPU\" in llm_device.value and \"qwen2-7b-instruct\" in llm_model_id.value:\n",
    "    ov_config[\"GPU_ENABLE_SDPA_OPTIMIZATION\"] = \"NO\"\n",
    "\n",
    "# On a GPU device a model is executed in FP16 precision. For red-pajama-3b-chat model there known accuracy\n",
    "# issues caused by this, which we avoid by setting precision hint to \"f32\".\n",
    "if llm_model_id.value == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and llm_device.value in [\"GPU\", \"AUTO\"]:\n",
    "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
    "\n",
    "llm = OpenVINOLLM(\n",
    "    model_name=str(model_dir),\n",
    "    tokenizer_name=str(model_dir),\n",
    "    context_window=3900,\n",
    "    max_new_tokens=2,\n",
    "    model_kwargs={\"ov_config\": ov_config, \"trust_remote_code\": True},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    device_map=llm_device.value,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/16155d31-c3e2-4b43-87f0-9b17c43aca6b",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
